agent.sources= r1
agent.sinks= k1
agent.channels= c1


agent.sources.r1.type= org.apache.flume.source.kafka.KafkaSource
agent.sources.r1.kafka.bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092
agent.sources.r1.kafka.topics=log-test
agent.sources.r1.kafka.consumer.group.id = flume-test
agent.sources.r1.flumeBatchSize=1000
agent.sources.r1.useFlumeEventFormat=false
agent.sources.r1.restart=true
agent.sources.r1.batchSize=1000
agent.sources.r1.batchTimeout=3000
agent.sources.r1.interceptors = i1
agent.sources.r1.interceptors.i1.type = com.tom.flume.interceptor.AppkeyInterceptor$Builder
agent.sources.r1.channels=c1

agent.channels.c1.type = file
agent.channels.c1.checkpointDir = /opt/module/checkpoint/kafka_collect1
agent.channels.c1.dataDirs = /opt/module/data/kafka_collect1
agent.channels.c1.maxFileSize = 104857600
agent.channels.c1.capacity = 90000000
agent.channels.c1.keep-alive = 60

agent.sinks.k1.channel=c1
agent.sinks.k1.type=hdfs
#日期放在前面，便于处理历史数据
agent.sinks.k1.hdfs.path=hdfs:///tmp/flume/%Y-%m-%d/%{appkey}
agent.sinks.k1.hdfs.writeFormat=Text
agent.sinks.k1.hdfs.rollSize=0
agent.sinks.k1.hdfs.rollCount=0
#这个单位是秒，时间过少会造成文件数量非常多
agent.sinks.k1.hdfs.rollInterval=300
agent.sinks.k1.hdfs.threadsPoolSize=30
agent.sinks.k1.hdfs.fileType = CompressedStream
agent.sinks.k1.hdfs.fileSuffix=.lzo
# 这个前缀必须要加，因为有多个flume会写入文件
agent.sinks.k1.hdfs.filePrefix=log1
agent.sinks.k1.hdfs.codeC = lzop